/* Copyright 2019 Axel Huebl, Benjamin Worpitz, Matthias Werner, Ren√© Widera
 *
 * This file is part of Alpaka.
 *
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/.
 */

#pragma once

#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

#include <alpaka/core/BoostPredef.hpp>

#if !BOOST_LANG_CUDA
    #error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
#endif

#include <alpaka/elem/Traits.hpp>
#include <alpaka/offset/Traits.hpp>
#include <alpaka/extent/Traits.hpp>
#include <alpaka/idx/Traits.hpp>
#include <alpaka/vec/Vec.hpp>
#include <alpaka/meta/Metafunctions.hpp>

// cuda_runtime_api.h: CUDA Runtime API C-style interface that does not require compiling with nvcc.
// cuda_runtime.h: CUDA Runtime API  C++-style interface built on top of the C API.
//  It wraps some of the C API routines, using overloading, references and default arguments.
//  These wrappers can be used from C++ code and can be compiled with any C++ compiler.
//  The C++ API also has some CUDA-specific wrappers that wrap C API routines that deal with symbols, textures, and device functions.
//  These wrappers require the use of \p nvcc because they depend on code being generated by the compiler.
//  For example, the execution configuration syntax to invoke kernels is only available in source code compiled with nvcc.
#include <cuda_runtime.h>
#include <cuda.h>

#include <array>
#include <type_traits>
#include <utility>
#include <iostream>
#include <string>
#include <stdexcept>
#include <cstddef>

#if (!defined(CUDART_VERSION) || (CUDART_VERSION < 9000))
    #error "CUDA version 9.0 or greater required!"
#endif

#if (!defined(CUDA_VERSION) || (CUDA_VERSION < 9000))
    #error "CUDA version 9.0 or greater required!"
#endif

namespace alpaka
{
    namespace cuda
    {
        namespace detail
        {
            //-----------------------------------------------------------------------------
            //! CUDA runtime API error checking with log and exception, ignoring specific error values
            ALPAKA_FN_HOST inline auto cudaRtCheck(
                cudaError_t const & error,
                char const * desc,
                char const * file,
                int const & line)
            -> void
            {
                if(error != cudaSuccess)
                {
                    std::string const sError(std::string(file) + "(" + std::to_string(line) + ") " + std::string(desc) + " : '" + cudaGetErrorName(error) +  "': '" + std::string(cudaGetErrorString(error)) + "'!");
#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
                    std::cerr << sError << std::endl;
#endif
                    ALPAKA_DEBUG_BREAK;
                    // reset the last error to allow user side error handling
                    cudaGetLastError();
                    throw std::runtime_error(sError);
                }
            }
            //-----------------------------------------------------------------------------
            //! CUDA runtime API error checking with log and exception, ignoring specific error values
            // NOTE: All ignored errors have to be convertible to cudaError_t.
            template<
                typename... TErrors>
            ALPAKA_FN_HOST auto cudaRtCheckIgnore(
                cudaError_t const & error,
                char const * cmd,
                char const * file,
                int const & line,
                TErrors && ... ignoredErrorCodes)
            -> void
            {
                if(error != cudaSuccess)
                {
                    std::array<cudaError_t, sizeof...(ignoredErrorCodes)> const aIgnoredErrorCodes{ignoredErrorCodes...};

                    // If the error code is not one of the ignored ones.
                    if(std::find(aIgnoredErrorCodes.cbegin(), aIgnoredErrorCodes.cend(), error) == aIgnoredErrorCodes.cend())
                    {
                        cudaRtCheck(error, ("'" + std::string(cmd) + "' returned error ").c_str(), file, line);
                    }
                }
            }
            //-----------------------------------------------------------------------------
            //! CUDA runtime API last error checking with log and exception.
            ALPAKA_FN_HOST inline auto cudaRtCheckLastError(
                char const * desc,
                char const * file,
                int const & line)
            -> void
            {
                cudaError_t const error(cudaGetLastError());
                cudaRtCheck(error, desc, file, line);
            }
        }
    }
}

#if BOOST_COMP_MSVC
    //-----------------------------------------------------------------------------
    //! CUDA runtime error checking with log and exception, ignoring specific error values
    #define ALPAKA_CUDA_RT_CHECK_IGNORE(cmd, ...)\
        ::alpaka::cuda::detail::cudaRtCheckLastError("'" #cmd "' A previous CUDA call (not this one) set the error ", __FILE__, __LINE__);\
        ::alpaka::cuda::detail::cudaRtCheckIgnore(cmd, #cmd, __FILE__, __LINE__, __VA_ARGS__)
#else
    #if BOOST_COMP_CLANG
        #pragma clang diagnostic push
        #pragma clang diagnostic ignored "-Wgnu-zero-variadic-macro-arguments"
    #endif
    //-----------------------------------------------------------------------------
    //! CUDA runtime error checking with log and exception, ignoring specific error values
    #define ALPAKA_CUDA_RT_CHECK_IGNORE(cmd, ...)\
        ::alpaka::cuda::detail::cudaRtCheckLastError("'" #cmd "' A previous CUDA call (not this one) set the error ", __FILE__, __LINE__);\
        ::alpaka::cuda::detail::cudaRtCheckIgnore(cmd, #cmd, __FILE__, __LINE__, ##__VA_ARGS__)
    #if BOOST_COMP_CLANG
        #pragma clang diagnostic pop
    #endif
#endif

//-----------------------------------------------------------------------------
//! CUDA runtime error checking with log and exception.
#define ALPAKA_CUDA_RT_CHECK(cmd)\
    ALPAKA_CUDA_RT_CHECK_IGNORE(cmd)

namespace alpaka
{
    namespace cuda
    {
        namespace detail
        {
            //-----------------------------------------------------------------------------
            //! CUDA driver API error checking with log and exception, ignoring specific error values
            ALPAKA_FN_HOST inline auto cudaDrvCheck(
                CUresult const & error,
                char const * desc,
                char const * file,
                int const & line)
            -> void
            {
                if(error == CUDA_SUCCESS)
                    return;

                char const * cu_err_name = nullptr;
                char const * cu_err_string = nullptr;
                CUresult cu_result_name = cuGetErrorName(error, &cu_err_name);
                CUresult cu_result_string = cuGetErrorString(error, &cu_err_string);
                std::string sError = std::string(file)
                                   + "(" + std::to_string(line) + ") "
                                   + std::string(desc) + " : '";
                if( cu_result_name == CUDA_SUCCESS && cu_result_string == CUDA_SUCCESS )
                {
                    sError += std::string(cu_err_name) +  "': '"
                            + std::string(cu_err_string) + "'!";
                } else {
                    // cuGetError*() failed, so append corresponding error message
                    if( cu_result_name == CUDA_ERROR_INVALID_VALUE ) {
                        sError += " cuGetErrorName: 'Invalid Value'!";
                    }
                    if( cu_result_string == CUDA_ERROR_INVALID_VALUE ) {
                        sError += " cuGetErrorString: 'Invalid Value'!";
                    }
                }
#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
                std::cerr << sError << std::endl;
#endif
                ALPAKA_DEBUG_BREAK;
                throw std::runtime_error(sError);
            }
        }
    }
}

//-----------------------------------------------------------------------------
//! CUDA driver error checking with log and exception.
#define ALPAKA_CUDA_DRV_CHECK(cmd)\
    ::alpaka::cuda::detail::cudaDrvCheck(cmd, #cmd, __FILE__, __LINE__)


//-----------------------------------------------------------------------------
// CUDA vector_types.h trait specializations.
namespace alpaka
{
    //-----------------------------------------------------------------------------
    //! The CUDA specifics.
    namespace cuda
    {
        namespace traits
        {
            //#############################################################################
            //! The CUDA vectors 1D dimension get trait specialization.
            template<
                typename T>
            struct IsCudaBuiltInType :
                std::integral_constant<
                    bool,
                    std::is_same<T, char1>::value
                    || std::is_same<T, double1>::value
                    || std::is_same<T, float1>::value
                    || std::is_same<T, int1>::value
                    || std::is_same<T, long1>::value
                    || std::is_same<T, longlong1>::value
                    || std::is_same<T, short1>::value
                    || std::is_same<T, uchar1>::value
                    || std::is_same<T, uint1>::value
                    || std::is_same<T, ulong1>::value
                    || std::is_same<T, ulonglong1>::value
                    || std::is_same<T, ushort1>::value
                    || std::is_same<T, char2>::value
                    || std::is_same<T, double2>::value
                    || std::is_same<T, float2>::value
                    || std::is_same<T, int2>::value
                    || std::is_same<T, long2>::value
                    || std::is_same<T, longlong2>::value
                    || std::is_same<T, short2>::value
                    || std::is_same<T, uchar2>::value
                    || std::is_same<T, uint2>::value
                    || std::is_same<T, ulong2>::value
                    || std::is_same<T, ulonglong2>::value
                    || std::is_same<T, ushort2>::value
                    || std::is_same<T, char3>::value
                    || std::is_same<T, dim3>::value
                    || std::is_same<T, double3>::value
                    || std::is_same<T, float3>::value
                    || std::is_same<T, int3>::value
                    || std::is_same<T, long3>::value
                    || std::is_same<T, longlong3>::value
                    || std::is_same<T, short3>::value
                    || std::is_same<T, uchar3>::value
                    || std::is_same<T, uint3>::value
                    || std::is_same<T, ulong3>::value
                    || std::is_same<T, ulonglong3>::value
                    || std::is_same<T, ushort3>::value
                    || std::is_same<T, char4>::value
                    || std::is_same<T, double4>::value
                    || std::is_same<T, float4>::value
                    || std::is_same<T, int4>::value
                    || std::is_same<T, long4>::value
                    || std::is_same<T, longlong4>::value
                    || std::is_same<T, short4>::value
                    || std::is_same<T, uchar4>::value
                    || std::is_same<T, uint4>::value
                    || std::is_same<T, ulong4>::value
                    || std::is_same<T, ulonglong4>::value
                    || std::is_same<T, ushort4>::value
// CUDA built-in variables have special types in clang native CUDA compilation
// defined in cuda_builtin_vars.h
#if BOOST_COMP_CLANG_CUDA
                    || std::is_same<T, __cuda_builtin_threadIdx_t>::value
                    || std::is_same<T, __cuda_builtin_blockIdx_t>::value
                    || std::is_same<T, __cuda_builtin_blockDim_t>::value
                    || std::is_same<T, __cuda_builtin_gridDim_t>::value
#endif
                >
            {};
        }
    }
    namespace dim
    {
        namespace traits
        {
            //#############################################################################
            //! The CUDA vectors 1D dimension get trait specialization.
            template<
                typename T>
            struct DimType<
                T,
                std::enable_if_t<
                    std::is_same<T, char1>::value
                    || std::is_same<T, double1>::value
                    || std::is_same<T, float1>::value
                    || std::is_same<T, int1>::value
                    || std::is_same<T, long1>::value
                    || std::is_same<T, longlong1>::value
                    || std::is_same<T, short1>::value
                    || std::is_same<T, uchar1>::value
                    || std::is_same<T, uint1>::value
                    || std::is_same<T, ulong1>::value
                    || std::is_same<T, ulonglong1>::value
                    || std::is_same<T, ushort1>::value
                >>
            {
                using type = dim::DimInt<1u>;
            };
            //#############################################################################
            //! The CUDA vectors 2D dimension get trait specialization.
            template<
                typename T>
            struct DimType<
                T,
                std::enable_if_t<
                    std::is_same<T, char2>::value
                    || std::is_same<T, double2>::value
                    || std::is_same<T, float2>::value
                    || std::is_same<T, int2>::value
                    || std::is_same<T, long2>::value
                    || std::is_same<T, longlong2>::value
                    || std::is_same<T, short2>::value
                    || std::is_same<T, uchar2>::value
                    || std::is_same<T, uint2>::value
                    || std::is_same<T, ulong2>::value
                    || std::is_same<T, ulonglong2>::value
                    || std::is_same<T, ushort2>::value
                >>
            {
                using type = dim::DimInt<2u>;
            };
            //#############################################################################
            //! The CUDA vectors 3D dimension get trait specialization.
            template<
                typename T>
            struct DimType<
                T,
                std::enable_if_t<
                    std::is_same<T, char3>::value
                    || std::is_same<T, dim3>::value
                    || std::is_same<T, double3>::value
                    || std::is_same<T, float3>::value
                    || std::is_same<T, int3>::value
                    || std::is_same<T, long3>::value
                    || std::is_same<T, longlong3>::value
                    || std::is_same<T, short3>::value
                    || std::is_same<T, uchar3>::value
                    || std::is_same<T, uint3>::value
                    || std::is_same<T, ulong3>::value
                    || std::is_same<T, ulonglong3>::value
                    || std::is_same<T, ushort3>::value
#if BOOST_COMP_CLANG_CUDA
                    || std::is_same<T, __cuda_builtin_threadIdx_t>::value
                    || std::is_same<T, __cuda_builtin_blockIdx_t>::value
                    || std::is_same<T, __cuda_builtin_blockDim_t>::value
                    || std::is_same<T, __cuda_builtin_gridDim_t>::value
#endif
                >>
            {
                using type = dim::DimInt<3u>;
            };
            //#############################################################################
            //! The CUDA vectors 4D dimension get trait specialization.
            template<
                typename T>
            struct DimType<
                T,
                std::enable_if_t<
                    std::is_same<T, char4>::value
                    || std::is_same<T, double4>::value
                    || std::is_same<T, float4>::value
                    || std::is_same<T, int4>::value
                    || std::is_same<T, long4>::value
                    || std::is_same<T, longlong4>::value
                    || std::is_same<T, short4>::value
                    || std::is_same<T, uchar4>::value
                    || std::is_same<T, uint4>::value
                    || std::is_same<T, ulong4>::value
                    || std::is_same<T, ulonglong4>::value
                    || std::is_same<T, ushort4>::value
                >>
            {
                using type = dim::DimInt<4u>;
            };
        }
    }
    namespace elem
    {
        namespace traits
        {
            //#############################################################################
            //! The CUDA vectors elem type trait specialization.
            template<
                typename T>
            struct ElemType<
                T,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<T>::value>>
            {
                using type = decltype(std::declval<T>().x);
            };
        }
    }
    namespace extent
    {
        namespace traits
        {
            //#############################################################################
            //! The CUDA vectors extent get trait specialization.
            template<
                typename TExtent>
            struct GetExtent<
                dim::DimInt<dim::Dim<TExtent>::value - 1u>,
                TExtent,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TExtent>::value
                    && (dim::Dim<TExtent>::value >= 1)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto getExtent(
                    TExtent const & extent)
                {
                    return extent.x;
                }
            };
            //#############################################################################
            //! The CUDA vectors extent get trait specialization.
            template<
                typename TExtent>
            struct GetExtent<
                dim::DimInt<dim::Dim<TExtent>::value - 2u>,
                TExtent,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TExtent>::value
                    && (dim::Dim<TExtent>::value >= 2)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto getExtent(
                    TExtent const & extent)
                {
                    return extent.y;
                }
            };
            //#############################################################################
            //! The CUDA vectors extent get trait specialization.
            template<
                typename TExtent>
            struct GetExtent<
                dim::DimInt<dim::Dim<TExtent>::value - 3u>,
                TExtent,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TExtent>::value
                    && (dim::Dim<TExtent>::value >= 3)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto getExtent(
                    TExtent const & extent)
                {
                    return extent.z;
                }
            };
            //#############################################################################
            //! The CUDA vectors extent get trait specialization.
            template<
                typename TExtent>
            struct GetExtent<
                dim::DimInt<dim::Dim<TExtent>::value - 4u>,
                TExtent,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TExtent>::value
                    && (dim::Dim<TExtent>::value >= 4)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto getExtent(
                    TExtent const & extent)
                {
                    return extent.w;
                }
            };
            //#############################################################################
            //! The CUDA vectors extent set trait specialization.
            template<
                typename TExtent,
                typename TExtentVal>
            struct SetExtent<
                dim::DimInt<dim::Dim<TExtent>::value - 1u>,
                TExtent,
                TExtentVal,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TExtent>::value
                    && (dim::Dim<TExtent>::value >= 1)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto setExtent(
                    TExtent const & extent,
                    TExtentVal const & extentVal)
                -> void
                {
                    extent.x = extentVal;
                }
            };
            //#############################################################################
            //! The CUDA vectors extent set trait specialization.
            template<
                typename TExtent,
                typename TExtentVal>
            struct SetExtent<
                dim::DimInt<dim::Dim<TExtent>::value - 2u>,
                TExtent,
                TExtentVal,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TExtent>::value
                    && (dim::Dim<TExtent>::value >= 2)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto setExtent(
                    TExtent const & extent,
                    TExtentVal const & extentVal)
                -> void
                {
                    extent.y = extentVal;
                }
            };
            //#############################################################################
            //! The CUDA vectors extent set trait specialization.
            template<
                typename TExtent,
                typename TExtentVal>
            struct SetExtent<
                dim::DimInt<dim::Dim<TExtent>::value - 3u>,
                TExtent,
                TExtentVal,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TExtent>::value
                    && (dim::Dim<TExtent>::value >= 3)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto setExtent(
                    TExtent const & extent,
                    TExtentVal const & extentVal)
                -> void
                {
                    extent.z = extentVal;
                }
            };
            //#############################################################################
            //! The CUDA vectors extent set trait specialization.
            template<
                typename TExtent,
                typename TExtentVal>
            struct SetExtent<
                dim::DimInt<dim::Dim<TExtent>::value - 4u>,
                TExtent,
                TExtentVal,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TExtent>::value
                    && (dim::Dim<TExtent>::value >= 4)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto setExtent(
                    TExtent const & extent,
                    TExtentVal const & extentVal)
                -> void
                {
                    extent.w = extentVal;
                }
            };
        }
    }
    namespace offset
    {
        namespace traits
        {
            //#############################################################################
            //! The CUDA vectors offset get trait specialization.
            template<
                typename TOffsets>
            struct GetOffset<
                dim::DimInt<dim::Dim<TOffsets>::value - 1u>,
                TOffsets,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TOffsets>::value
                    && (dim::Dim<TOffsets>::value >= 1)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto getOffset(
                    TOffsets const & offsets)
                {
                    return offsets.x;
                }
            };
            //#############################################################################
            //! The CUDA vectors offset get trait specialization.
            template<
                typename TOffsets>
            struct GetOffset<
                dim::DimInt<dim::Dim<TOffsets>::value - 2u>,
                TOffsets,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TOffsets>::value
                    && (dim::Dim<TOffsets>::value >= 2)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto getOffset(
                    TOffsets const & offsets)
                {
                    return offsets.y;
                }
            };
            //#############################################################################
            //! The CUDA vectors offset get trait specialization.
            template<
                typename TOffsets>
            struct GetOffset<
                dim::DimInt<dim::Dim<TOffsets>::value - 3u>,
                TOffsets,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TOffsets>::value
                    && (dim::Dim<TOffsets>::value >= 3)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto getOffset(
                    TOffsets const & offsets)
                {
                    return offsets.z;
                }
            };
            //#############################################################################
            //! The CUDA vectors offset get trait specialization.
            template<
                typename TOffsets>
            struct GetOffset<
                dim::DimInt<dim::Dim<TOffsets>::value - 4u>,
                TOffsets,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TOffsets>::value
                    && (dim::Dim<TOffsets>::value >= 4)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto getOffset(
                    TOffsets const & offsets)
                {
                    return offsets.w;
                }
            };
            //#############################################################################
            //! The CUDA vectors offset set trait specialization.
            template<
                typename TOffsets,
                typename TOffset>
            struct SetOffset<
                dim::DimInt<dim::Dim<TOffsets>::value - 1u>,
                TOffsets,
                TOffset,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TOffsets>::value
                    && (dim::Dim<TOffsets>::value >= 1)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto setOffset(
                    TOffsets const & offsets,
                    TOffset const & offset)
                -> void
                {
                    offsets.x = offset;
                }
            };
            //#############################################################################
            //! The CUDA vectors offset set trait specialization.
            template<
                typename TOffsets,
                typename TOffset>
            struct SetOffset<
                dim::DimInt<dim::Dim<TOffsets>::value - 2u>,
                TOffsets,
                TOffset,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TOffsets>::value
                    && (dim::Dim<TOffsets>::value >= 2)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto setOffset(
                    TOffsets const & offsets,
                    TOffset const & offset)
                -> void
                {
                    offsets.y = offset;
                }
            };
            //#############################################################################
            //! The CUDA vectors offset set trait specialization.
            template<
                typename TOffsets,
                typename TOffset>
            struct SetOffset<
                dim::DimInt<dim::Dim<TOffsets>::value - 3u>,
                TOffsets,
                TOffset,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TOffsets>::value
                    && (dim::Dim<TOffsets>::value >= 3)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto setOffset(
                    TOffsets const & offsets,
                    TOffset const & offset)
                -> void
                {
                    offsets.z = offset;
                }
            };
            //#############################################################################
            //! The CUDA vectors offset set trait specialization.
            template<
                typename TOffsets,
                typename TOffset>
            struct SetOffset<
                dim::DimInt<dim::Dim<TOffsets>::value - 4u>,
                TOffsets,
                TOffset,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TOffsets>::value
                    && (dim::Dim<TOffsets>::value >= 4)>>
            {
                ALPAKA_NO_HOST_ACC_WARNING
                ALPAKA_FN_HOST_ACC static auto setOffset(
                    TOffsets const & offsets,
                    TOffset const & offset)
                -> void
                {
                    offsets.w = offset;
                }
            };
        }
    }
    namespace idx
    {
        namespace traits
        {
            //#############################################################################
            //! The CUDA vectors idx type trait specialization.
            template<
                typename TIdx>
            struct IdxType<
                TIdx,
                std::enable_if_t<
                    cuda::traits::IsCudaBuiltInType<TIdx>::value>>
            {
                using type = std::size_t;
            };
        }
    }
}

#endif
