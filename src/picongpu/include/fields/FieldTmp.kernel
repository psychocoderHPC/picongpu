/**
 * Copyright 2013-2015 Axel Huebl, Rene Widera, Marco Garten, Benjamin Worpitz
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */



#pragma once

#include "types.h"
#include "particles/frame_types.hpp"

#include "simulation_defines.hpp"

#include "FieldTmp.hpp"
#include "particles/memory/boxes/ParticlesBox.hpp"

#include "memory/boxes/CachedBox.hpp"
#include "dimensions/DataSpaceOperations.hpp"
#include "nvidia/functors/Add.hpp"
#include "mappings/threads/ThreadCollective.hpp"
#include "algorithms/Set.hpp"

#include "particles/frame_types.hpp"

namespace picongpu
{
    using namespace PMacc;

    template<
        typename BlockDescription_,
        uint32_t AREA>
    struct KernelComputeSupercells
    {
    template<
        typename T_Acc,
        typename TmpBox,
        typename ParBox,
        typename FrameSolver,
        typename Mapping>
    ALPAKA_FN_ACC void operator()(
        T_Acc const & acc,
        TmpBox const & fieldTmp,
        ParBox const & boxPar,
        FrameSolver const & frameSolver,
        Mapping const & mapper) const
    {
        DataSpace<simDim> const blockIndex(alpaka::idx::getIdx<alpaka::Grid, alpaka::Blocks>(acc));
        DataSpace<simDim> const threadIndex(alpaka::idx::getIdx<alpaka::Block, alpaka::Threads>(acc));

        typedef typename BlockDescription_::SuperCellSize SuperCellSize;
        const DataSpace<simDim> block( mapper.getSuperCellIndex( DataSpace<simDim > (blockIndex) ) );

        const int linearThreadIdx = DataSpaceOperations<simDim>::template map<SuperCellSize > ( threadIndex );

        auto frame(alpaka::block::shared::allocVar<typename ParBox::FrameType *>(acc));
        auto isValid(alpaka::block::shared::allocVar<bool>(acc));
        auto particlesInSuperCell(alpaka::block::shared::allocVar<lcellId_t>(acc));

        /* wait until all shared memory is initialised */
        alpaka::block::sync::syncBlockThreads(acc);

        if( linearThreadIdx == 0 )
        {
            frame = &( boxPar.getLastFrame( block, isValid ) );
            particlesInSuperCell = boxPar.getSuperCell( block ).getSizeLastFrame( );
        }
        alpaka::block::sync::syncBlockThreads(acc);

        if( !isValid )
            return; //end kernel if we have no frames

        auto cachedVal(CachedBox::create < 0, typename TmpBox::ValueType > (acc, BlockDescription_( ) ) );
        Set<typename TmpBox::ValueType > set( float_X( 0.0 ) );

        ThreadCollective<BlockDescription_> collective( linearThreadIdx );
        collective( set, cachedVal );

        alpaka::block::sync::syncBlockThreads(acc);
        while( isValid )
        {
            if( linearThreadIdx < particlesInSuperCell )
            {
                frameSolver( *frame, linearThreadIdx, SuperCellSize::toRT(), cachedVal );
            }
            alpaka::block::sync::syncBlockThreads(acc);
            if( linearThreadIdx == 0 )
            {
                frame = &( boxPar.getPreviousFrame( *frame, isValid ) );
                particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
            }
            alpaka::block::sync::syncBlockThreads(acc);
        }

        nvidia::functors::Add add;
        const DataSpace<simDim> blockCell = block * MappingDesc::SuperCellSize::toRT( );
        PMACC_AUTO( fieldTmpBlock, fieldTmp.shift( blockCell ) );
        collective( add, fieldTmpBlock, cachedVal );
        alpaka::block::sync::syncBlockThreads(acc);
    }
    };

    struct KernelBashValue
    {
    template<
        typename T_Acc,
        typename Box,
        typename Mapping>
    ALPAKA_FN_ACC void operator()(
        T_Acc const & acc,
        Box const & fieldTmp,
        Box const & targetJ,
        DataSpace<simDim> const & exchangeSize,
        DataSpace<simDim> const & direction,
        Mapping const & mapper) const
    {
        DataSpace<simDim> const blockIndex(alpaka::idx::getIdx<alpaka::Grid, alpaka::Blocks>(acc));
        DataSpace<simDim> const threadIndex(alpaka::idx::getIdx<alpaka::Block, alpaka::Threads>(acc));

        const DataSpace<simDim> blockCell(
                                           mapper.getSuperCellIndex( DataSpace<simDim > (blockIndex) )
                                           * Mapping::SuperCellSize::toRT( )
                                           );
        const DataSpace<Mapping::Dim> sourceCell( blockCell + threadIndex );

        /*origin in area from local GPU*/
        DataSpace<simDim> nullSourceCell(
                                          mapper.getSuperCellIndex( DataSpace<simDim > ( ) )
                                          * Mapping::SuperCellSize::toRT( )
                                          );
        DataSpace<simDim> targetCell( sourceCell - nullSourceCell );

        for (uint32_t d = 0; d < simDim; ++d)
        {
            if( direction[d] == -1 )
            {
                if( threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d] ) return;
                targetCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
            }
            else if( ( direction[d] == 1 ) && ( threadIndex[d] >= exchangeSize[d] ) ) return;
        }
        targetJ( targetCell ) = fieldTmp( sourceCell );
    }
    };

    struct KernelInsertValue
    {
    template<
        typename T_Acc,
        typename Box,
        typename Mapping>
    ALPAKA_FN_ACC void operator()(
        T_Acc const & acc,
        Box const & fieldTmp,
        Box const & sourceTmp,
        DataSpace<simDim> const & exchangeSize,
        DataSpace<simDim> const & direction,
        Mapping const & mapper) const
    {
        DataSpace<simDim> const blockIndex(alpaka::idx::getIdx<alpaka::Grid, alpaka::Blocks>(acc));
        DataSpace<simDim> const threadIndex(alpaka::idx::getIdx<alpaka::Block, alpaka::Threads>(acc));

        const DataSpace<simDim> blockCell(
                                           mapper.getSuperCellIndex( DataSpace<simDim > (blockIndex) )
                                           * Mapping::SuperCellSize::toRT( )
                                           );
        DataSpace<Mapping::Dim> targetCell( blockCell + threadIndex );

        /*origin in area from local GPU*/
        DataSpace<simDim> nullSourceCell(
                                          mapper.getSuperCellIndex( DataSpace<simDim > ( ) )
                                          * Mapping::SuperCellSize::toRT( )
                                          );
        DataSpace<simDim> sourceCell( targetCell - nullSourceCell );

        for (uint32_t d = 0; d < simDim; ++d)
        {
            if( direction[d] == 1 )
            {
                if( threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d] ) return;
                sourceCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
                targetCell[d] -= SuperCellSize::toRT()[d];
            }
            else if( direction[d] == -1 )
            {
                if( threadIndex[d] >= exchangeSize[d] ) return;
                targetCell[d] += SuperCellSize::toRT()[d];
            }
        }

        fieldTmp( targetCell ) += sourceTmp( sourceCell );
    }
    };

} // namespace picongpu
