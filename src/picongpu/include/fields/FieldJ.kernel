/**
 * Copyright 2013-2017 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten,
 *                     Benjamin Worpitz
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "pmacc_types.hpp"
#include "particles/frame_types.hpp"

#include "simulation_defines.hpp"

#include "FieldJ.hpp"
#include "particles/memory/boxes/ParticlesBox.hpp"


#include "algorithms/Velocity.hpp"

#include "memory/boxes/CachedBox.hpp"
#include "dimensions/DataSpaceOperations.hpp"
#include "nvidia/functors/Add.hpp"
#include "mappings/threads/ThreadCollective.hpp"
#include "algorithms/Set.hpp"
#include "mappings/threads/ForEachIdx.hpp"
#include "mappings/threads/IdxConfig.hpp"
#include "memory/CtxArray.hpp"
#include "particles/frame_types.hpp"

namespace picongpu
{

using namespace PMacc;

typedef FieldJ::DataBoxType J_DataBox;

template< uint32_t T_worker, class T_BlockDescription, uint32_t AREA >
struct KernelComputeCurrent
{
    template<class JBox, class ParBox, class Mapping, class FrameSolver>
    DINLINE void operator()(JBox fieldJ,
                                         ParBox boxPar, FrameSolver frameSolver, Mapping mapper) const
    {
        using namespace mappings::threads;

        typedef typename ParBox::FrameType FrameType;
        typedef typename ParBox::FramePtr FramePtr;
        typedef typename Mapping::SuperCellSize SuperCellSize;

        constexpr uint32_t cellsPerSuperCell = PMacc::math::CT::volume< SuperCellSize >::type::value;
        constexpr uint32_t numWorker = T_worker;
        /* `workerMultiplier > 1` means that we had more worker than cells in a superCell */
        constexpr uint32_t workerMultiplier = ( numWorker + cellsPerSuperCell - 1 ) / cellsPerSuperCell;

        uint32_t const workerIdx = threadIdx.x;

        using VirtualWorkerDomCfg = IdxConfig<
            cellsPerSuperCell * workerMultiplier,
            numWorker
        >;

        memory::CtxArray<
            uint32_t,
            VirtualWorkerDomCfg
        >
        virtualBlockIdCtx(
            workerIdx,
            [&]( uint32_t const linearIdx, uint32_t const )
            {
                return ( linearIdx ) / cellsPerSuperCell;
            }
        );

        memory::CtxArray<
            uint32_t,
            VirtualWorkerDomCfg
        >
        virtualLinearIdCtx(
            workerIdx,
            [&]( uint32_t const linearIdx, uint32_t const idx )
            {
                /* map virtualLinearIdCtx to the range [0;cellsPerSuperCell) */
                return linearIdx - ( virtualBlockIdCtx[ idx ] * cellsPerSuperCell );
            }
        );

        const DataSpace<simDim> block(
            mapper.getSuperCellIndex(
                DataSpace< simDim >( blockIdx )
            )
        );

        memory::CtxArray<
            FramePtr,
            VirtualWorkerDomCfg
        > frameCtx;

        memory::CtxArray<
            lcellId_t,
            VirtualWorkerDomCfg
        > particlesInSuperCellCtx( 0 );

        /* loop over all virtual worker */
        ForEachIdx< VirtualWorkerDomCfg > forEachVirtualWorker( workerIdx );

        forEachVirtualWorker(
            [&]( uint32_t const, uint32_t const idx )
            {
                frameCtx[ idx ] = boxPar.getLastFrame( block );
                if( frameCtx[ idx ].isValid() && virtualBlockIdCtx[ idx ] == 0 )
                    particlesInSuperCellCtx[ idx ] = boxPar.getSuperCell( block ).getSizeLastFrame();

                /* select N-th (N=virtualBlockId) frame from the end of the list*/
                for( int i = 1; ( i <= virtualBlockIdCtx[ idx ] ) && frameCtx[ idx ].isValid(); ++i )
                {
                    particlesInSuperCellCtx[ idx ] = cellsPerSuperCell;
                    frameCtx[ idx ] = boxPar.getPreviousFrame( frameCtx[ idx ] );
                }
            }
        );

        /* This memory is used by all virtual blocks*/
        auto cachedJ = CachedBox::create <
            0,
            typename JBox::ValueType
        >( T_BlockDescription() );

        Set<typename JBox::ValueType > set( float3_X::create( 0.0 ) );
        ThreadCollective<
            T_BlockDescription,
            numWorker
        > collectiveSet( workerIdx );

        collectiveSet( set, cachedJ );

        __syncthreads();

        forEachVirtualWorker(
            [&]( uint32_t const, uint32_t const idx )
            {
                while( frameCtx[ idx ].isValid() )
                {
                    /* this test is only important for the last frame
                     * if frame is not the last one particlesInSuperCell==particles count in superCell
                     */
                    if( virtualLinearIdCtx[ idx ] < particlesInSuperCellCtx[ idx ] )
                    {
                        frameSolver(
                            *frameCtx[ idx ],
                            virtualLinearIdCtx[ idx ],
                            cachedJ
                        );
                    }

                    particlesInSuperCellCtx[ idx ] = cellsPerSuperCell;
                    for( int i = 0; ( i < workerMultiplier ) && frameCtx[ idx ].isValid(); ++i )
                    {
                        frameCtx[ idx ] = boxPar.getPreviousFrame( frameCtx[ idx ] );
                    }
                }
            }
        );

        /* we wait that all worker finish the loop*/
        __syncthreads();

        nvidia::functors::Add add;
        DataSpace< simDim > const blockCell = block * SuperCellSize::toRT();
        ThreadCollective<
            T_BlockDescription,
            numWorker
        > collectiveAdd( workerIdx );
        auto fieldJBlock = fieldJ.shift( blockCell );
        collectiveAdd( add, fieldJBlock, cachedJ );
    }
};

template<class ParticleAlgo, class Velocity, class TVec>
struct ComputeCurrentPerFrame
{

    HDINLINE ComputeCurrentPerFrame(const float_X deltaTime) :
    m_deltaTime(deltaTime)
    {
    }

    template<class FrameType, class BoxJ >
    DINLINE void operator()(FrameType& frame, const int localIdx, BoxJ & jBox)
    {

        auto particle = frame[localIdx];
        const float_X weighting = particle[weighting_];
        const floatD_X pos = particle[position_];
        const int particleCellIdx = particle[localCellIdx_];
        const float_X charge = attribute::getCharge(weighting,particle);
        const DataSpace<simDim> localCell(DataSpaceOperations<simDim>::template map<TVec > (particleCellIdx));

        Velocity velocity;
        const float3_X vel = velocity(
                                      particle[momentum_],
                                      attribute::getMass(weighting,particle));
        auto fieldJShiftToParticle = jBox.shift(localCell);
        ParticleAlgo perParticle;
        perParticle(fieldJShiftToParticle,
                    pos,
                    vel,
                    charge,
                    m_deltaTime
                    );
    }

private:
    const PMACC_ALIGN(m_deltaTime, float_32);
};

struct KernelAddCurrentToEMF
{
    template<class T_CurrentInterpolation, class T_Mapping>
    DINLINE void operator()(typename FieldE::DataBoxType fieldE,
                                          typename FieldB::DataBoxType fieldB,
                                          J_DataBox fieldJ,
                                          T_CurrentInterpolation currentInterpolation,
                                          T_Mapping mapper) const
    {
        /* Caching of fieldJ */
        typedef SuperCellDescription<
                    SuperCellSize,
                    typename T_CurrentInterpolation::LowerMargin,
                    typename T_CurrentInterpolation::UpperMargin
                    > BlockArea;

        auto cachedJ = CachedBox::create < 0, typename J_DataBox::ValueType > (BlockArea());

        nvidia::functors::Assign assign;
        const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
        const DataSpace<simDim> blockCell = block * MappingDesc::SuperCellSize::toRT();

        const DataSpace<simDim > threadIndex(threadIdx);
        auto fieldJBlock = fieldJ.shift(blockCell);

        ThreadCollective<BlockArea> collective(threadIndex);
        collective(
                  assign,
                  cachedJ,
                  fieldJBlock
                  );

        __syncthreads();

        const DataSpace<T_Mapping::Dim> cell(blockCell + threadIndex);

        // Amperes Law:
        //   Change of the dE = - j / EPS0 * dt
        //                        j = current density (= current per area)
        //                          = fieldJ
        currentInterpolation( fieldE.shift(cell), fieldB.shift(cell), cachedJ.shift(threadIndex) );
    }
};

template< uint32_t T_worker >
struct KernelBashCurrent
{
    template<class Mapping>
    DINLINE void operator()(J_DataBox fieldJ,
                                      J_DataBox targetJ,
                                      DataSpace<simDim> exchangeSize,
                                      DataSpace<simDim> direction,
                                      Mapping mapper) const
    {
        using namespace mappings::threads;

        /* number of cells in a superCell */
        constexpr uint32_t numCells = PMacc::math::CT::volume< SuperCellSize >::type::value;
        constexpr uint32_t numWorker = T_worker;

        const uint32_t workerIdx = threadIdx.x;

        const DataSpace<simDim> blockCell(
            mapper.getSuperCellIndex( DataSpace<simDim > (blockIdx) )
                * Mapping::SuperCellSize::toRT()
        );

        /*origin in area from local GPU*/
        DataSpace<simDim> nullSourceCell(
            mapper.getSuperCellIndex( DataSpace<simDim > () )
                * Mapping::SuperCellSize::toRT()
        );

        ForEachIdx<
            IdxConfig<
                numCells,
                numWorker
            >
        >{ workerIdx }(
            [&]( uint32_t const linearIdx, uint32_t const )
            {
                /* cell index within the superCell */
                DataSpace< simDim > cellIdx = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );

                DataSpace<Mapping::Dim> const sourceCell( blockCell + cellIdx );
                DataSpace<simDim> targetCell( sourceCell - nullSourceCell );

                bool assignValue = true;

                for( uint32_t d = 0; d < simDim; ++d )
                {
                    if( direction[ d ] == -1 )
                    {
                        if( cellIdx[ d ] < SuperCellSize::toRT()[ d ] - exchangeSize[ d ] )
                            assignValue = false;
                        targetCell[ d ] -= SuperCellSize::toRT()[ d ] - exchangeSize[ d ];
                    }
                    else if( (direction[d] == 1) && ( cellIdx[d] >= exchangeSize[d] ) )
                        assignValue = false;
                }

                if( assignValue )
                    targetJ( targetCell ) = fieldJ( sourceCell );
            }
        );
    }
};

template< uint32_t T_worker >
struct KernelInsertCurrent
{
    template<class Mapping>
    DINLINE void operator()(J_DataBox fieldJ,
                                        J_DataBox sourceJ,
                                        DataSpace<simDim> exchangeSize,
                                        DataSpace<simDim> direction,
                                        Mapping mapper) const
    {
        using namespace mappings::threads;

        /* number of cells in a superCell */
        constexpr uint32_t numCells = PMacc::math::CT::volume< SuperCellSize >::type::value;
        constexpr uint32_t numWorker = T_worker;

        const uint32_t workerIdx = threadIdx.x;

        const DataSpace<simDim> blockCell(
            mapper.getSuperCellIndex( DataSpace<simDim >( blockIdx ) )
                * Mapping::SuperCellSize::toRT()
        );

        /*origin in area from local GPU*/
        DataSpace<simDim> nullSourceCell(
            mapper.getSuperCellIndex( DataSpace< simDim > () )
            * Mapping::SuperCellSize::toRT()
        );

        ForEachIdx<
            IdxConfig<
                numCells,
                numWorker
            >
        >{ workerIdx }(
            [&]( uint32_t const linearIdx, uint32_t const )
            {
                /* cell index within the superCell */
                DataSpace< simDim > cellIdx = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );
                DataSpace< simDim > targetCell( blockCell + cellIdx );
                DataSpace< simDim > sourceCell( targetCell - nullSourceCell );

                bool assignValue = true;

                for( uint32_t d = 0; d < simDim; ++d )
                {
                    if( direction[ d ] == 1 )
                    {
                        if( cellIdx[ d ] < SuperCellSize::toRT()[ d ] - exchangeSize[ d ] )
                            assignValue = false;
                        sourceCell[ d ] -= SuperCellSize::toRT()[ d ] - exchangeSize[ d ];
                        targetCell[ d ] -= SuperCellSize::toRT()[ d ];
                    }
                    else if( direction[ d ] == -1 )
                    {
                        if( cellIdx[ d ] >= exchangeSize[ d ] )
                            assignValue = false;
                        targetCell[ d ] += SuperCellSize::toRT()[ d ];
                    }
                }
                if( assignValue )
                    fieldJ( targetCell ) += sourceJ( sourceCell );
            }
        );
    }
};

}
