/* Copyright 2014-2021 Rene Widera, Alexander Grund
 *
 * This file is part of PMacc.
 *
 * PMacc is free software: you can redistribute it and/or modify
 * it under the terms of either the GNU General Public License or
 * the GNU Lesser General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PMacc is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License and the GNU Lesser General Public License
 * for more details.
 *
 * You should have received a copy of the GNU General Public License
 * and the GNU Lesser General Public License along with PMacc.
 * If not, see <http://www.gnu.org/licenses/>.
 */


#pragma once

#include "pmacc/debug/VerboseLog.hpp"
#include "pmacc/dimensions/DataSpaceOperations.hpp"
#include "pmacc/kernel/atomic.hpp"
#include "pmacc/mappings/threads/ForEachIdx.hpp"
#include "pmacc/mappings/threads/IdxConfig.hpp"
#include "pmacc/math/Vector.hpp"
#include "pmacc/memory/Array.hpp"
#include "pmacc/memory/CtxVar.hpp"
#include "pmacc/memory/shared/Allocate.hpp"
#include "pmacc/particles/frame_types.hpp"
#include "pmacc/traits/GetNumWorkers.hpp"
#include "pmacc/types.hpp"

namespace pmacc
{
    namespace particles
    {
        namespace operations
        {
            namespace detail
            {
                /** transform a large frame into a list of small frames
                 *
                 * @tparam T_numWorkers number of workers
                 */
                template<uint32_t T_numWorkers>
                struct KernelSplitIntoListOfFrames
                {
                    /** Copy particles from big frame to PMacc frame structure
                     *  (Opposite to ConcatListOfFrames)
                     *
                     * - convert a user-defined domainCellIdx to localCellIdx
                     * - processed particles per block <= number of cells per superCell
                     *
                     * @tparam T_CounterBox pmacc:DataBox, type of buffer for the statistics counter
                     * @tparam T_DestBox pmacc:ParticlesBox, type of the destination particle box
                     * @tparam T_SrcFrame pmacc:Frame, type of the source frame
                     * @tparam T_Space pmacc::DataSpace, type for indicies and offsets within the domain
                     * @tparam T_Identifier Identifier, type of the identifier for the total domain offset
                     * @tparam T_CellDescription pmacc::MappingDescription, type of the domain description
                     * @tparam T_Acc alpaka accelerator type
                     *
                     * @param acc alpaka accelerator
                     * @param counter box with three integers [sharedSrcParticleOffset, numLoadedParticles,
                     * numUsedFrames]
                     * @param destBox particle box where all particles are copied to (destination)
                     * @param srcFrame frame with particles (is used as source)
                     * @param maxParticles number of particles in srcFrame
                     * @param localDomainCellOffset offset in cells to user-defined domain (@see wiki PIConGPU domain
                     * definitions)
                     * @param domainCellIdxIdentifier the identifier for the particle domain cellIdx
                     *                                that is calculated back to the local domain
                     *                                with respect to localDomainCellOffset
                     * @param cellDesc supercell domain description
                     */
                    template<
                        typename T_CounterBox,
                        typename T_DestBox,
                        typename T_SrcFrame,
                        typename T_Space,
                        typename T_Identifier,
                        typename T_CellDescription,
                        typename T_Acc>
                    DINLINE void operator()(
                        T_Acc const& acc,
                        T_CounterBox counter,
                        T_DestBox destBox,
                        T_SrcFrame srcFrame,
                        int const maxParticles,
                        T_Space const localDomainCellOffset,
                        T_Identifier const domainCellIdxIdentifier,
                        T_CellDescription const cellDesc) const
                    {
                        using namespace pmacc::particles::operations;
                        using namespace mappings::threads;

                        using SrcFrameType = T_SrcFrame;
                        using DestFrameType = typename T_DestBox::FrameType;
                        using DestFramePtr = typename T_DestBox::FramePtr;
                        using SuperCellSize = typename DestFrameType::SuperCellSize;

                        constexpr uint32_t numWorkers = T_numWorkers;
                        constexpr uint32_t numDims = T_DestBox::Dim;
                        constexpr uint32_t particlesPerFrame = math::CT::volume<SuperCellSize>::type::value;

                        PMACC_SMEM(acc, destFramePtr, memory::Array<DestFramePtr, particlesPerFrame>);
                        PMACC_SMEM(acc, sharedLinearSuperCellIds, memory::Array<int, particlesPerFrame>);
                        PMACC_SMEM(acc, sharedSrcParticleOffset, int);

                        uint32_t const workerIdx = cupla::threadIdx(acc).x;

                        DataSpace<numDims> const numSuperCells(
                            cellDesc.getGridSuperCells() - cellDesc.getGuardingSuperCells() * 2);

                        ForEachIdx<IdxConfig<1, numWorkers>> onlyMaster{workerIdx};

                        onlyMaster([&]() {
                            /* apply for work for the full block, counter[0] contains the
                             * offset in srcFrame to load N particles
                             */
                            sharedSrcParticleOffset = cupla::atomicAdd(
                                acc,
                                &(counter[0]),
                                particlesPerFrame,
                                ::alpaka::hierarchy::Blocks{});
                        });

                        cupla::__syncthreads(acc);

                        using ParticleDomCfg = IdxConfig<particlesPerFrame, numWorkers>;

                        memory::CtxVar<int, ParticleDomCfg> srcParticleIdxCtx{};

                        memory::CtxVar<bool, ParticleDomCfg> hasValidParticleCtx{};

                        // loop over all particles in the frame
                        ForEachIdx<ParticleDomCfg> forEachParticle(workerIdx);

                        forEachParticle([&](DomainIdx const domIdx) {
                            destFramePtr[domIdx.lIdx()] = DestFramePtr{};
                            sharedLinearSuperCellIds[domIdx.lIdx()] = -1;

                            srcParticleIdxCtx[domIdx] = sharedSrcParticleOffset + domIdx.lIdx();
                            hasValidParticleCtx[domIdx] = srcParticleIdxCtx[domIdx] < maxParticles;
                        });

                        cupla::__syncthreads(acc);

                        // supercell index of the particle relative to the origin of the local domain
                        memory::CtxVar<DataSpace<numDims>, ParticleDomCfg> particlesSuperCellCtx{};

                        // linear cell index of the particle within the destination frame
                        memory::CtxVar<lcellId_t, ParticleDomCfg> lCellIdxCtx(INV_LOC_IDX);

                        memory::CtxVar<int, ParticleDomCfg> linearParticlesSuperCellCtx(-1);

                        forEachParticle([&](DomainIdx const domIdx) {
                            if(hasValidParticleCtx[domIdx])
                            {
                                // offset of the particle relative to the origin of the local domain
                                DataSpace<numDims> const particleCellOffset
                                    = srcFrame[srcParticleIdxCtx[domIdx]][domainCellIdxIdentifier]
                                    - localDomainCellOffset;
                                particlesSuperCellCtx[domIdx] = particleCellOffset / SuperCellSize::toRT();
                                linearParticlesSuperCellCtx[domIdx]
                                    = DataSpaceOperations<numDims>::map(numSuperCells, particlesSuperCellCtx[domIdx]);
                                sharedLinearSuperCellIds[domIdx.lIdx()] = linearParticlesSuperCellCtx[domIdx];
                                DataSpace<numDims> const localCellIdx(
                                    particleCellOffset - particlesSuperCellCtx[domIdx] * SuperCellSize::toRT());
                                lCellIdxCtx[domIdx]
                                    = DataSpaceOperations<numDims>::template map<SuperCellSize>(localCellIdx);
                            }
                        });

                        cupla::__syncthreads(acc);

                        memory::CtxVar<int, ParticleDomCfg> masterVirtualThreadIdxCtx(
                            workerIdx,
                            [&](uint32_t const linearIdx) { return linearIdx - 1; });

                        forEachParticle([&](DomainIdx const domIdx) {
                            if(hasValidParticleCtx[domIdx])
                            {
                                auto& vThreadMasterIdx = masterVirtualThreadIdxCtx[domIdx];
                                /* search master thread index */
                                while(vThreadMasterIdx >= 0)
                                {
                                    if(linearParticlesSuperCellCtx[domIdx]
                                       != sharedLinearSuperCellIds[vThreadMasterIdx])
                                        break;

                                    --vThreadMasterIdx;
                                }
                                ++vThreadMasterIdx;

                                // load empty frame if virtual thread is the master
                                if(vThreadMasterIdx == domIdx.lIdx())
                                {
                                    /* counter[2] -> number of used frames */
                                    kernel::atomicAllInc(acc, &(counter[2]), ::alpaka::hierarchy::Blocks{});
                                    DestFramePtr tmpFrame = destBox.getEmptyFrame(acc);
                                    destFramePtr[domIdx.lIdx()] = tmpFrame;
                                    destBox.setAsFirstFrame(
                                        acc,
                                        tmpFrame,
                                        particlesSuperCellCtx[domIdx] + cellDesc.getGuardingSuperCells());
                                }
                            }
                        });

                        cupla::__syncthreads(acc);

                        forEachParticle([&](DomainIdx const domIdx) {
                            if(hasValidParticleCtx[domIdx])
                            {
                                /* copy attributes and activate particle*/
                                auto parDest = destFramePtr[masterVirtualThreadIdxCtx[domIdx]][domIdx.lIdx()];
                                auto parDestDeselect = deselect<bmpl::vector2<localCellIdx, multiMask>>(parDest);

                                assign(parDestDeselect, srcFrame[srcParticleIdxCtx[domIdx]]);
                                parDest[localCellIdx_] = lCellIdxCtx[domIdx];
                                parDest[multiMask_] = 1;
                                /* counter[1] -> number of loaded particles
                                 * this counter is evaluated on host side
                                 * (check that loaded particles by this kernel == loaded particles from HDF5 file)*/
                                kernel::atomicAllInc(acc, &(counter[1]), ::alpaka::hierarchy::Blocks{});
                            }
                        });
                    }
                };
            } // namespace detail

            /** Copy particles from big frame to PMacc frame structure
             *  (Opposite to ConcatListOfFrames)
             *
             * - convert a user-defined domainCellIdx to localCellIdx
             * - processed particles per block <= number of cells per superCell
             *
             * @tparam T_LogLvl type of the loc level for debuging output
             * @tparam T_DestSpecies pmacc:ParticlesBase, type of the destination species
             * @tparam T_SrcFrame pmacc:ParticlesBox, type of the source particle frame
             * @tparam T_Space pmacc::DataSpace, type for indicies and offsets within the domain
             * @tparam T_Identifier Identifier, type of the identifier for the total domain offset
             * @tparam T_CellDescription pmacc::MappingDescription, type of the domain description
             *
             * @param destSpecies particle species instance whose deviceBuffer is written
             * @param srcFrame device frame with particles (is used as source)
             * @param numParticles number of particles in srcFrame
             * @param chunkSize number of particles to process in one kernel call
             * @param localDomainCellOffset offset in cells to user-defined domain (@see wiki PIConGPU domain
             * definitions)
             * @param domainCellIdxIdentifier the identifier for the particle domain cellIdx
             *                                that is calculated back to the local domain
             *                                with respect to localDomainCellOffset
             * @param cellDesc supercell domain description
             * @param logLvl Log level used for information logging
             */
            template<
                typename T_LogLvl,
                typename T_DestSpecies,
                typename T_SrcFrame,
                typename T_Space,
                typename T_Identifier,
                typename T_CellDescription>
            HINLINE void splitIntoListOfFrames(
                T_DestSpecies& destSpecies,
                T_SrcFrame srcFrame,
                uint32_t numParticles,
                uint32_t const chunkSize,
                T_Space const& localDomainCellOffset,
                T_Identifier const domainCellIdxIdentifier,
                T_CellDescription const& cellDesc,
                T_LogLvl const& logLvl = T_LogLvl())
            {
                using SuperCellSize = typename T_CellDescription::SuperCellSize;
                uint32_t const cellsInSuperCell = pmacc::math::CT::volume<SuperCellSize>::type::value;

                /* counter is used to apply for work, count used frames and count loaded particles
                 * [0] -> offset for loading particles
                 * [1] -> number of loaded particles
                 * [2] -> number of used frames
                 *
                 * all values are zero after initialization
                 */
                GridBuffer<uint32_t, DIM1> counterBuffer(DataSpace<DIM1>(3));

                uint32_t const iterationsForLoad
                    = math::float2int_ru(static_cast<double>(numParticles) / static_cast<double>(chunkSize));
                uint32_t leftOverParticles = numParticles;

                for(uint32_t i = 0; i < iterationsForLoad; ++i)
                {
                    /* only load a chunk of particles per iteration to avoid blow up of frame usage */
                    uint32_t currentChunkSize = std::min(leftOverParticles, chunkSize);
                    log(logLvl, "load particles on device chunk offset=%1%; chunk size=%2%; left particles %3%")
                        % (i * chunkSize) % currentChunkSize % leftOverParticles;

                    constexpr uint32_t numWorkers
                        = pmacc::traits::GetNumWorkers<pmacc::math::CT::volume<SuperCellSize>::type::value>::value;

                    PMACC_KERNEL(detail::KernelSplitIntoListOfFrames<numWorkers>{})
                    (math::float2int_ru(double(currentChunkSize) / double(cellsInSuperCell)), numWorkers)(
                        counterBuffer.getDeviceBuffer().getDataBox(),
                        destSpecies.getDeviceParticlesBox(),
                        srcFrame,
                        static_cast<int>(numParticles),
                        localDomainCellOffset,
                        domainCellIdxIdentifier,
                        cellDesc);
                    destSpecies.fillAllGaps();
                    leftOverParticles -= currentChunkSize;
                }

                counterBuffer.deviceToHost();
                log(logLvl, "wait for last processed chunk: %1%") % T_SrcFrame::getName();

                __getTransactionEvent().waitForFinished();

                log(logLvl, "used frames to load particles: %1%") % counterBuffer.getHostBuffer().getDataBox()[2];

                if(static_cast<uint64_t>(counterBuffer.getHostBuffer().getDataBox()[1]) != numParticles)
                {
                    log(logLvl, "error load species | counter is %1% but should %2%")
                        % counterBuffer.getHostBuffer().getDataBox()[1] % numParticles;
                    throw std::runtime_error("Failed to load expected number of particles to GPU.");
                }
            }

        } // namespace operations
    } // namespace particles
} // namespace pmacc
