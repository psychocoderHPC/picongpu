/* Copyright 2013-2022 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten,
 *                     Benjamin Worpitz
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "picongpu/simulation_defines.hpp"

#include "picongpu/algorithms/Velocity.hpp"
#include "picongpu/fields/FieldJ.hpp"
#include "picongpu/fields/currentDeposition/Cache.hpp"
#include "picongpu/fields/currentDeposition/Strategy.def"

#include <pmacc/dimensions/DataSpaceOperations.hpp>
#include <pmacc/lockstep.hpp>
#include <pmacc/mappings/threads/ThreadCollective.hpp>
#include <pmacc/math/operation.hpp>
#include <pmacc/memory/boxes/CachedBox.hpp>
#include <pmacc/particles/algorithm/ForEach.hpp>
#include <pmacc/particles/frame_types.hpp>
#include <pmacc/particles/memory/boxes/ParticlesBox.hpp>
#include <pmacc/types.hpp>

#include <type_traits>
#include <utility>

namespace picongpu
{
    namespace currentSolver
    {
        /** compute current
         *
         * @tparam T_numWorkers number of workers
         * @tparam T_BlockDescription current field domain description needed for the
         *                            collective stencil
         */
        template<uint32_t T_numWorkers, typename T_BlockDescription>
        struct KernelComputeCurrent
        {
            /** scatter particle current of particles located in a supercell
             *
             * The current for the supercell including the guards is cached in shared memory
             * and scattered at the end of the functor to the global memory.
             *
             * @tparam JBox pmacc::DataBox, particle current box type
             * @tparam ParBox pmacc::ParticlesBox, particle box type
             * @tparam Mapping mapper functor type
             * @tparam FrameSolver frame solver functor type
             * @param T_Acc alpaka accelerator type
             *
             * @param alpaka accelerator
             * @param fieldJ field with particle current
             * @param boxPar particle memory
             * @param frameSolver functor to calculate the current for a frame
             * @param mapper functor to map a block to a supercell
             */
            template<typename JBox, typename ParBox, typename FrameSolver, typename Mapping, typename T_Acc>
            DINLINE void operator()(
                T_Acc const& acc,
                JBox fieldJ,
                ParBox boxPar,
                FrameSolver frameSolver,
                Mapping mapper) const
            {
                using SuperCellSize = typename Mapping::SuperCellSize;

                constexpr uint32_t numWorkers = T_numWorkers;

                const DataSpace<simDim> superCellIdx(
                    mapper.getSuperCellIndex(DataSpace<simDim>(cupla::blockIdx(acc))));
                uint32_t const workerIdx = cupla::threadIdx(acc).x;

                auto forEachParticle
                    = pmacc::particles::algorithm::acc::makeForEach<numWorkers>(workerIdx, boxPar, superCellIdx);

                // end kernel if we have no particles
                if(!forEachParticle.hasParticles())
                    return;

                DataSpace<simDim> const blockCell = superCellIdx * SuperCellSize::toRT();
                using Strategy = currentSolver::traits::GetStrategy_t<FrameSolver>;

                /* this memory is used by all virtual blocks */
                auto cachedJ = detail::Cache<Strategy>::template create<numWorkers, T_BlockDescription>(
                    acc,
                    fieldJ.shift(blockCell),
                    workerIdx);

                cupla::__syncthreads(acc);

                forEachParticle(
                    acc,
                    [&cachedJ, &frameSolver](auto const& accelerator, auto& particle)
                    { frameSolver(accelerator, particle, cachedJ); });

                /* we wait that all workers finish the loop */
                cupla::__syncthreads(acc);

                /* this memory is used by all virtual blocks */
                detail::Cache<Strategy>::template flush<numWorkers, T_BlockDescription>(
                    acc,
                    fieldJ.shift(blockCell),
                    cachedJ,
                    workerIdx);
            }
        };

        template<typename T_ParticleAlgo, typename Velocity, typename TVec>
        struct ComputePerFrame
        {
            using ParticleAlgo = T_ParticleAlgo;

            HDINLINE ComputePerFrame(const float_X deltaTime) : m_deltaTime(deltaTime)
            {
            }

            template<typename T_Particle, typename BoxJ, typename T_Acc>
            DINLINE void operator()(T_Acc const& acc, T_Particle& particle, BoxJ& jBox)
            {
                const float_X weighting = particle[weighting_];
                const floatD_X pos = particle[position_];
                const int particleCellIdx = particle[localCellIdx_];
                const float_X charge = attribute::getCharge(weighting, particle);
                const DataSpace<simDim> localCell(DataSpaceOperations<simDim>::template map<TVec>(particleCellIdx));

                Velocity velocity;
                const float3_X vel = velocity(particle[momentum_], attribute::getMass(weighting, particle));
                auto fieldJShiftToParticle = jBox.shift(localCell);
                ParticleAlgo perParticle;
                perParticle(acc, fieldJShiftToParticle, pos, vel, charge, m_deltaTime);
            }

        private:
            PMACC_ALIGN(m_deltaTime, const float_32);
        };

        namespace traits
        {
            template<typename ParticleAlgo, typename Velocity, typename TVec>
            struct GetStrategy<ComputePerFrame<ParticleAlgo, Velocity, TVec>>
            {
                using type = GetStrategy_t<ParticleAlgo>;
            };
        } // namespace traits

        /** add current to electric and magnetic field
         *
         * @tparam T_numWorkers number of workers
         */
        template<uint32_t T_numWorkers>
        struct KernelAddCurrentToEMF
        {
            template<typename T_CurrentInterpolationFunctor, typename T_Mapping, typename T_Acc>
            DINLINE void operator()(
                T_Acc const& acc,
                typename FieldE::DataBoxType fieldE,
                typename FieldB::DataBoxType fieldB,
                typename FieldJ::DataBoxType fieldJ,
                T_CurrentInterpolationFunctor currentInterpolationFunctor,
                T_Mapping mapper) const
            {
                /* Caching of fieldJ */
                using BlockArea = SuperCellDescription<
                    SuperCellSize,
                    typename T_CurrentInterpolationFunctor::LowerMargin,
                    typename T_CurrentInterpolationFunctor::UpperMargin>;

                constexpr uint32_t cellsPerSuperCell = pmacc::math::CT::volume<SuperCellSize>::type::value;
                constexpr uint32_t numWorkers = T_numWorkers;

                uint32_t const workerIdx = cupla::threadIdx(acc).x;

                auto cachedJ = CachedBox::create<0, typename FieldJ::DataBoxType::ValueType>(acc, BlockArea());

                pmacc::math::operation::Assign assign;
                DataSpace<simDim> const block(mapper.getSuperCellIndex(DataSpace<simDim>(cupla::blockIdx(acc))));
                DataSpace<simDim> const blockCell = block * MappingDesc::SuperCellSize::toRT();


                auto fieldJBlock = fieldJ.shift(blockCell);

                ThreadCollective<BlockArea, numWorkers> collective(workerIdx);

                collective(acc, assign, cachedJ, fieldJBlock);

                cupla::__syncthreads(acc);

                lockstep::makeForEach<cellsPerSuperCell, numWorkers>(workerIdx)(
                    [&](uint32_t const linearIdx)
                    {
                        /* cell index within the superCell */
                        DataSpace<simDim> const cellIdx
                            = DataSpaceOperations<simDim>::template map<SuperCellSize>(linearIdx);
                        DataSpace<simDim> const cell(blockCell + cellIdx);

                        // Amperes Law:
                        //   Change of the dE = - j / EPS0 * dt
                        //                        j = current density (= current per area)
                        //                          = fieldJ
                        currentInterpolationFunctor(fieldE.shift(cell), fieldB.shift(cell), cachedJ.shift(cellIdx));
                    });
            }
        };

    } // namespace currentSolver
} // namespace picongpu
